{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "import math\n",
    "from sklearn.externals import joblib\n",
    "import torch\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dir =  'data/preprocessed'\n",
    "\n",
    "data = joblib.load(input_dir+'/fine.pkl')\n",
    "\n",
    "train,dev,test = data['data']\n",
    "\n",
    "word2id = data['dicts']['word2id']\n",
    "\n",
    "id2word = data['dicts']['id2word']\n",
    "\n",
    "embd = data['embd']\n",
    "\n",
    "input_dim = len(embd[0])\n",
    "\n",
    "max_len_train = max([len(d) for d in train[0]])\n",
    "\n",
    "max_len_dev = max([len(d) for d in dev[0]])\n",
    "\n",
    "max_len_test = max([len(d) for d in test[0]])\n",
    "\n",
    "max_len = max([max_len_train,max_len_dev,max_len_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import random\n",
    "\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self,data,batch_size,max_len):\n",
    "        \n",
    "        self.xs = data[0]\n",
    "        \n",
    "        self.ys = data[1]\n",
    "        \n",
    "        self.max_length = max_len\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.batch_num = 0\n",
    "        \n",
    "        self.num_of_samples = len(self.xs)\n",
    "        \n",
    "        self.max_batch_num = int(self.num_of_samples / self.batch_size) \n",
    "        \n",
    "        self.indexes = np.arange(len(data[0]))\n",
    "        \n",
    "\n",
    "    def next(self):\n",
    "        \n",
    "        X = np.zeros((self.batch_size,self.max_length),dtype=np.int32)\n",
    "        \n",
    "        Y = np.zeros((self.batch_size),dtype=np.int32)\n",
    "        \n",
    "        lengths = np.zeros((self.batch_size),dtype=np.int32)    \n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            index = self.indexes[self.batch_num * self.batch_size + i]\n",
    "            \n",
    "            lengths[i] = len(self.xs[index])\n",
    "\n",
    "            X[i,:lengths[i]] = self.xs[index]\n",
    "            \n",
    "            Y[i] = self.ys[index]\n",
    "            \n",
    "        self.batch_num = (self.batch_num + 1) % self.max_batch_num\n",
    "\n",
    "        res = [X,Y,lengths]\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def shuffle(self):\n",
    "        \n",
    "        np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n"
     ]
    }
   ],
   "source": [
    "train_batcher = Batcher(train,25,max_len)\n",
    "\n",
    "dev_batcher = Batcher(dev,25,max_len)\n",
    "\n",
    "test_batcher = Batcher(test,25,max_len)\n",
    "\n",
    "model = Model(5,max_len,len(embd),embd,emb_dim =input_dim)\n",
    "\n",
    "for epoch in range(15):\n",
    "    \n",
    "    train_batcher.shuffle()\n",
    "    \n",
    "    step_per_epoch = train_batcher.max_batch_num\n",
    "\n",
    "    loss = 0.0\n",
    "    \n",
    "    print \"Epoch %d\" % epoch\n",
    "        \n",
    "    for i in range(step_per_epoch):\n",
    "        \n",
    "        input_x, y, lengths = train_batcher.next()\n",
    "    \n",
    "    step_per_epoch = dev_batcher.max_batch_num\n",
    "    \n",
    "    for i in range(step_per_epoch):\n",
    "        \n",
    "        input_x, y, lengths = dev_batcher.next()\n",
    "\n",
    "    step_per_epoch = test_batcher.max_batch_num\n",
    "        \n",
    "    for i in range(step_per_epoch):\n",
    "        \n",
    "        input_x, y, lengths = test_batcher.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def _lengths_to_masks(lengths, max_length):\n",
    "    \n",
    "    tiled_ranges = autograd.Variable(torch.arange(0,float(max_length)).unsqueeze(0).expand([len(lengths),max_length]))\n",
    "    \n",
    "    lengths = lengths.float().unsqueeze(1).expand_as(tiled_ranges)\n",
    "    \n",
    "    mask = tiled_ranges.lt(lengths).float()\n",
    "\n",
    "    return mask\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \n",
    "    initial = np.random.uniform(-0.01, 0.01,shape)\n",
    "    \n",
    "    initial = torch.from_numpy(initial)\n",
    "    \n",
    "    return initial.float()\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self,num_classes,max_length,num_tokens,embd,emb_dim = 300,hidden_dim=100,lr=0.001):\n",
    "        \n",
    "        super(Model,self).__init__()\n",
    "    \n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.att_dim = 50\n",
    "        \n",
    "        self.depth = 10\n",
    "        \n",
    "        self.num_att = num_classes\n",
    "        \n",
    "            \n",
    "        self.embedding = nn.Embedding(num_tokens,emb_dim)\n",
    "        \n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(embd),requires_grad=False)\n",
    "        \n",
    "        self.lstm_fw = nn.LSTMCell(self.emb_dim ,self.hidden_dim)\n",
    "        \n",
    "        self.lstm_bw = nn.LSTMCell(self.emb_dim ,self.hidden_dim)\n",
    "\n",
    "        self.loss_fn = nn.functional.cross_entropy\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.err = 1e-24\n",
    "        \n",
    "        self.optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
    " \n",
    "    def init_hidden(self,batch_size):\n",
    "        \n",
    "        return (autograd.Variable(torch.zeros(batch_size, self.hidden_dim)),autograd.Variable(torch.zeros(batch_size, self.hidden_dim)))\n",
    "        \n",
    "    def bidirectional_forward(self,x,lengths):\n",
    "        \n",
    "        max_length = x.size()[1]\n",
    "                \n",
    "        mask =  _lengths_to_masks(lengths,max_length)\n",
    "        \n",
    "        x_embd =  self.dropout(self.embedding(x).transpose(0,1))\n",
    "                \n",
    "        hidden_fw = self.init_hidden(len(x))\n",
    "        \n",
    "        hidden_bw = self.init_hidden(len(x))\n",
    "        \n",
    "        lstm_fw_outputs = []\n",
    "        \n",
    "        lstm_bw_outputs = []\n",
    "        \n",
    "        for i in range(self.max_length): \n",
    "        \n",
    "            hidden_fw = self.lstm_fw(x_embd[i],hidden_fw)\n",
    "        \n",
    "            hidden_fw = [fw * mask[:,i].unsqueeze(1).expand_as(fw) for fw in hidden_fw]\n",
    "            \n",
    "            \n",
    "            hidden_bw = self.lstm_bw(x_embd[-i-1],hidden_bw)\n",
    "            \n",
    "            hidden_bw = [bw * mask[:,-i-1].unsqueeze(1).expand_as(bw) for bw in hidden_bw]\n",
    "            \n",
    "                        \n",
    "            lstm_fw_outputs.append(hidden_fw[0])\n",
    "            \n",
    "            lstm_bw_outputs.append(hidden_bw[0])\n",
    "        \n",
    "        lstm_bw_outputs = lstm_bw_outputs[::-1]\n",
    "        \n",
    "        lstm_outputs = torch.cat([torch.cat([fw,bw],1).unsqueeze(1) for fw,bw in zip(lstm_fw_outputs,lstm_bw_outputs)],1)\n",
    "        \n",
    "        return self.dropout(lstm_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_x, y, lengths =train_batcher.next()\n",
    "input_x = autograd.Variable(torch.from_numpy(input_x).long())\n",
    "        \n",
    "y = autograd.Variable(torch.from_numpy(y)).long()\n",
    "\n",
    "lengths = autograd.Variable(torch.from_numpy(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "  0.0420 -0.0886 -0.0133  ...  -0.1151 -0.1539  0.0280\n",
       "  0.0589  0.1427 -0.1632  ...  -0.0000 -0.1333  0.0603\n",
       "  0.0928  0.0844 -0.1697  ...  -0.0388 -0.0634 -0.0179\n",
       "           ...             ⋱             ...          \n",
       " -0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       " -0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "\n",
       "( 1 ,.,.) = \n",
       "  0.0021  0.0399  0.0584  ...   0.1026 -0.3495  0.0499\n",
       "  0.0143  0.1123  0.1419  ...   0.0789 -0.0000  0.0716\n",
       "  0.1315 -0.1021  0.0743  ...   0.0670 -0.3094 -0.0663\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "\n",
       "( 2 ,.,.) = \n",
       "  0.1015  0.1227 -0.0728  ...  -0.1120 -0.2880  0.0255\n",
       "  0.0804  0.0752 -0.0941  ...  -0.0585 -0.1575  0.0322\n",
       "  0.1611  0.0836 -0.0000  ...  -0.1543 -0.1319 -0.0710\n",
       "           ...             ⋱             ...          \n",
       " -0.0000  0.0000  0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000 -0.0000  ...   0.0000 -0.0000 -0.0000\n",
       "... \n",
       "\n",
       "(22 ,.,.) = \n",
       " -0.0394 -0.0057  0.2208  ...  -0.0576 -0.1683  0.1019\n",
       "  0.0028  0.1074  0.0196  ...  -0.0056 -0.2168  0.2071\n",
       "  0.1664  0.0000 -0.0335  ...  -0.1253 -0.1727  0.2099\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       " -0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       " -0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(23 ,.,.) = \n",
       "  0.0000 -0.0218 -0.0752  ...   0.0723 -0.1781 -0.0033\n",
       "  0.1726  0.0487 -0.0866  ...  -0.0000 -0.1908 -0.0437\n",
       "  0.2209  0.2266 -0.1766  ...  -0.1313 -0.1379 -0.0577\n",
       "           ...             ⋱             ...          \n",
       "  0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000  0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "\n",
       "(24 ,.,.) = \n",
       "  0.0501  0.0700  0.1029  ...  -0.0726 -0.1526 -0.0091\n",
       "  0.0431  0.1242  0.0281  ...  -0.0831 -0.1432 -0.1194\n",
       " -0.0513  0.0000 -0.0225  ...  -0.0648 -0.0865  0.0657\n",
       "           ...             ⋱             ...          \n",
       " -0.0000  0.0000  0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "  0.0000  0.0000 -0.0000  ...  -0.0000 -0.0000 -0.0000\n",
       "[torch.FloatTensor of size 25x56x200]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(5,max_len,len(embd),embd,emb_dim =input_dim)\n",
    "\n",
    "model.bidirectional_forward(input_x,lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5451, 7073, 7074, ...,    0,    0,    0],\n",
       "       [ 372,  161,  284, ...,    0,    0,    0],\n",
       "       [1338, 9908,   23, ...,    0,    0,    0],\n",
       "       ..., \n",
       "       [   0, 4402,    9, ...,    0,    0,    0],\n",
       "       [   6,   88,  377, ...,    0,    0,    0],\n",
       "       [3654, 3808,    2, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
