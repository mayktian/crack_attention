{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "import math\n",
    "from sklearn.externals import joblib\n",
    "import torch\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir =  'data/preprocessed'\n",
    "\n",
    "data = joblib.load(input_dir+'/fine.pkl')\n",
    "\n",
    "train,dev,test = data['data']\n",
    "\n",
    "word2id = data['dicts']['word2id']\n",
    "\n",
    "id2word = data['dicts']['id2word']\n",
    "\n",
    "embd = data['embd']\n",
    "\n",
    "input_dim = len(embd[0])\n",
    "\n",
    "max_len_train = max([len(d) for d in train[0]])\n",
    "\n",
    "max_len_dev = max([len(d) for d in dev[0]])\n",
    "\n",
    "max_len_test = max([len(d) for d in test[0]])\n",
    "\n",
    "max_len = max([max_len_train,max_len_dev,max_len_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import random\n",
    "\n",
    "\n",
    "class Batcher:\n",
    "    def __init__(self,data,batch_size,max_len):\n",
    "        \n",
    "        self.xs = data[0]\n",
    "        \n",
    "        self.ys = data[1]\n",
    "        \n",
    "        self.max_length = max_len\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.batch_num = 0\n",
    "        \n",
    "        self.num_of_samples = len(self.xs)\n",
    "        \n",
    "        self.max_batch_num = int(self.num_of_samples / self.batch_size) \n",
    "        \n",
    "        self.indexes = np.arange(len(data[0]))\n",
    "        \n",
    "\n",
    "    def next(self):\n",
    "        \n",
    "        X = np.zeros((self.batch_size,self.max_length),dtype=np.int32)\n",
    "        \n",
    "        Y = np.zeros((self.batch_size),dtype=np.int32)\n",
    "        \n",
    "        lengths = np.zeros((self.batch_size),dtype=np.int32)    \n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            index = self.indexes[self.batch_num * self.batch_size + i]\n",
    "            \n",
    "            lengths[i] = len(self.xs[index])\n",
    "\n",
    "            X[i,:lengths[i]] = self.xs[index]\n",
    "            \n",
    "            Y[i] = self.ys[index]\n",
    "            \n",
    "        self.batch_num = (self.batch_num + 1) % self.max_batch_num\n",
    "\n",
    "        res = [X,Y,lengths]\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def shuffle(self):\n",
    "        \n",
    "        np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "dev acc: 35.78\n",
      "test acc: 36.89\n",
      "Epoch 1\n",
      "dev acc: 35.50\n",
      "test acc: 39.97\n",
      "Epoch 2\n",
      "dev acc: 35.05\n",
      "test acc: 39.51\n",
      "Epoch 3\n",
      "dev acc: 39.27\n",
      "test acc: 41.37\n",
      "Epoch 4\n",
      "dev acc: 38.17\n",
      "test acc: 42.72\n",
      "Epoch 5\n",
      "dev acc: 37.98\n",
      "test acc: 39.51\n",
      "Epoch 6\n",
      "dev acc: 39.72\n",
      "test acc: 43.61\n",
      "Epoch 7\n",
      "dev acc: 39.72\n",
      "test acc: 44.78\n",
      "Epoch 8\n"
     ]
    }
   ],
   "source": [
    "train_batcher = Batcher(train,25,max_len)\n",
    "\n",
    "dev_batcher = Batcher(dev,109,max_len)\n",
    "\n",
    "test_batcher = Batcher(test,67,max_len)\n",
    "\n",
    "model = Model(5,max_len,len(embd),embd,emb_dim =input_dim,hidden_dim=120)\n",
    "\n",
    "def hook(outputs,ys):\n",
    "    \n",
    "    corr = 0\n",
    "    for output,y in zip(outputs,ys):\n",
    "        pred = np.argmax(output)\n",
    "        if pred == y :\n",
    "            corr +=1 \n",
    "    return corr,len(ys)\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "best_dev_acc = 0\n",
    "for epoch in range(30):\n",
    "    \n",
    "    train_batcher.shuffle()\n",
    "    \n",
    "    step_per_epoch = train_batcher.max_batch_num\n",
    "\n",
    "    loss = 0.0\n",
    "    \n",
    "    print \"Epoch %d\" % epoch\n",
    "        \n",
    "    for i in range(step_per_epoch):\n",
    "        \n",
    "        input_x, y, lengths = train_batcher.next()\n",
    "        \n",
    "        input_x = autograd.Variable(torch.from_numpy(input_x).long())\n",
    "        \n",
    "        y = autograd.Variable(torch.from_numpy(y)).long()\n",
    "\n",
    "        lengths = autograd.Variable(torch.from_numpy(lengths))\n",
    "        \n",
    "        loss += model.train_(input_x,y,lengths)\n",
    "            \n",
    "    step_per_epoch = dev_batcher.max_batch_num\n",
    "    corr = 0\n",
    "    total = 0\n",
    "    for i in range(step_per_epoch):\n",
    "        \n",
    "        input_x, y, lengths = dev_batcher.next()\n",
    "        \n",
    "        input_x = autograd.Variable(torch.from_numpy(input_x).long())\n",
    "        \n",
    "        y = autograd.Variable(torch.from_numpy(y)).long()\n",
    "\n",
    "        lengths = autograd.Variable(torch.from_numpy(lengths))\n",
    "                \n",
    "        outputs  = model.test(input_x,lengths)\n",
    "\n",
    "        corr_,total_ = hook(outputs,y.data.numpy())\n",
    "        \n",
    "        corr += corr_\n",
    "        total += total_\n",
    "        \n",
    "    dev_acc = (float(corr)*100/total)\n",
    "    print 'dev acc: %.2f' % dev_acc\n",
    "    \n",
    "    \n",
    "    step_per_epoch = test_batcher.max_batch_num\n",
    "    corr = 0\n",
    "    total = 0\n",
    "    for i in range(step_per_epoch):\n",
    "        \n",
    "        input_x, y, lengths = test_batcher.next()\n",
    "        \n",
    "        input_x = autograd.Variable(torch.from_numpy(input_x).long())\n",
    "        \n",
    "        y = autograd.Variable(torch.from_numpy(y)).long()\n",
    "\n",
    "        lengths = autograd.Variable(torch.from_numpy(lengths))\n",
    "        \n",
    "        outputs  = model.test(input_x,lengths)\n",
    "\n",
    "        corr_,total_ = hook(outputs,y.data.numpy())\n",
    "        \n",
    "        corr += corr_\n",
    "        total += total_\n",
    "        \n",
    "    test_acc = (float(corr) * 100 / total)\n",
    "    print 'test acc: %.2f' % test_acc\n",
    "    \n",
    "    if dev_acc > best_dev_acc:\n",
    "        best_acc = test_acc\n",
    "        \n",
    "print \"best test acc:%.2f\" % (best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def _lengths_to_masks(lengths, max_length):\n",
    "    \n",
    "    tiled_ranges = autograd.Variable(torch.arange(0,float(max_length)).unsqueeze(0).expand([len(lengths),max_length]))\n",
    "    \n",
    "    lengths = lengths.float().unsqueeze(1).expand_as(tiled_ranges)\n",
    "    \n",
    "    mask = tiled_ranges.lt(lengths).float()\n",
    "\n",
    "    return mask\n",
    "\n",
    "# def gather(indexes, max_length,data):\n",
    "    \n",
    "\n",
    "def weight_variable(shape):\n",
    "    \n",
    "    initial = np.random.uniform(-0.01, 0.01,shape)\n",
    "    \n",
    "    initial = torch.from_numpy(initial)\n",
    "    \n",
    "    return initial.float()\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self,num_classes,max_length,num_tokens,embd,emb_dim = 300,hidden_dim=100,lr=0.05):\n",
    "        \n",
    "        super(Model,self).__init__()\n",
    "    \n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "                        \n",
    "        self.num_att = num_classes\n",
    "        \n",
    "            \n",
    "        self.embedding = nn.Embedding(num_tokens,emb_dim)\n",
    "        \n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(embd),requires_grad=True)\n",
    "        \n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, self.num_classes)\n",
    "        \n",
    "        self.lstm_fw = nn.LSTMCell(self.emb_dim ,self.hidden_dim)\n",
    "        \n",
    "        self.lstm_bw = nn.LSTMCell(self.emb_dim ,self.hidden_dim)\n",
    "\n",
    "        self.loss_fn = nn.functional.cross_entropy\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.err = 1e-24\n",
    "        \n",
    "        self.optimizer = optim.Adagrad(filter(lambda p: p.requires_grad, self.parameters()), lr=lr,weight_decay=1e-4)\n",
    " \n",
    "    def init_hidden(self,batch_size):\n",
    "        \n",
    "        return (autograd.Variable(torch.zeros(batch_size, self.hidden_dim)),autograd.Variable(torch.zeros(batch_size, self.hidden_dim)))\n",
    "        \n",
    "    def bidirectional_forward(self,x,lengths):\n",
    "        \n",
    "        max_length = x.size()[1]\n",
    "        \n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        mask =  _lengths_to_masks(lengths,max_length)\n",
    "        \n",
    "        x_embd =  self.dropout(self.embedding(x).transpose(0,1))\n",
    "                \n",
    "        hidden_fw = self.init_hidden(len(x))\n",
    "        \n",
    "        hidden_bw = self.init_hidden(len(x))\n",
    "        \n",
    "        lstm_fw_outputs = []\n",
    "        \n",
    "        lstm_bw_outputs = []\n",
    "        indices =  autograd.Variable(torch.arange(0,float(batch_size))*max_length).long() + (lengths-1).long()\n",
    "        \n",
    "        for i in range(self.max_length): \n",
    "        \n",
    "            hidden_fw = self.lstm_fw(x_embd[i],hidden_fw)\n",
    "        \n",
    "            hidden_fw = [self.dropout(fw) * mask[:,i].unsqueeze(1).expand_as(fw) for fw in hidden_fw]\n",
    "            \n",
    "            hidden_bw = self.lstm_bw(x_embd[-i-1],hidden_bw)\n",
    "            \n",
    "            hidden_bw = [self.dropout(bw) * mask[:,-i-1].unsqueeze(1).expand_as(bw) for bw in hidden_bw]\n",
    "            \n",
    "                        \n",
    "            lstm_fw_outputs.append(hidden_fw[0])\n",
    "            \n",
    "            lstm_bw_outputs.append(hidden_bw[0])\n",
    "        \n",
    "        lstm_bw_outputs = lstm_bw_outputs[::-1]\n",
    "        \n",
    "        lstm_fw_output = torch.cat([fw.unsqueeze(1) for fw in lstm_fw_outputs],1).view(-1,self.hidden_dim).index_select(0,indices)\n",
    "        \n",
    "        lstm_bw_output =  lstm_bw_outputs[0]\n",
    "        \n",
    "        \n",
    "        lstm_outputs = torch.cat([lstm_fw_output,lstm_bw_output],-1)\n",
    "        \n",
    "        return lstm_outputs\n",
    "    \n",
    "    def classify(self, outputs):\n",
    "        \n",
    "        return self.softmax(self.linear(outputs))\n",
    "    \n",
    "    def train_(self,x,y,lengths):\n",
    "        \n",
    "        self.zero_grad()\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        lstm_outputs = self.bidirectional_forward(x,lengths)\n",
    "        \n",
    "        output_ = self.classify(lstm_outputs)\n",
    "        \n",
    "        y = y.view(-1)\n",
    "        \n",
    "        loss = self.loss_fn(output_,y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "    def test(self,x,lengths):\n",
    "        \n",
    "        self.eval()\n",
    "        \n",
    "        lstm_outputs = self.bidirectional_forward(x,lengths)\n",
    "        \n",
    "              \n",
    "\n",
    "        output_ = self.classify(lstm_outputs)\n",
    "\n",
    "        return output_.data.numpy()#,c_att,global_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
